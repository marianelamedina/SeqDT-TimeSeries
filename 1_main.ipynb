{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree for Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_dataset import load_dataset\n",
    "\n",
    "from gini import * #Gini, improvement_gini\n",
    "from projected_dataset import project_dataset\n",
    "from upper_bound import * #upper_bound, split_node\n",
    "from DiscoverPatternHighestGini import DPH\n",
    "from BestTree import * #majority, BT\n",
    "from PessimisticErrorPruning import * #node_error, subtree_error, standard_error, PEP\n",
    "from SeqDT import * #SeqDT, sequences_per_class, calculate_tree_depth\n",
    "\n",
    "from Representation import plot_tree\n",
    "\n",
    "from ModelEvaluation import * #compute_metrics, predict_sequence, evaluate_model, evaluate_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found in dataset:  ['+1', '-1']\n",
      "[(['b', 'a', 'b', 'c', 'd'], '+1'), (['c', 'a', 'b', 'e'], '+1'), (['a', 'd', 'e', 'c', 'b'], '+1'), (['a', 'd', 'e', 'c', 'b'], '-1'), (['a', 'd', 'e', 'c', 'b'], '-1'), (['d', 'b', 'd', 'b'], '-1'), (['c', 'a', 'e'], '-1')]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"Datasets/dataset_paper.txt\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini index\n",
    "\n",
    "The Gini coefficient measures the probability of a particular variable being wrongly classified when it is randomly chosen, ranging from 0 (pure: all samples belong to one class) to 1 (maximum impurity: uniform class distribution)\n",
    "\n",
    "$Gini(y(T)) = 1 - \\sum_{i=1}^d p_i^2$\n",
    "\n",
    "where $p_i$ is the proportion of samples in node $T$ belonging to class $i$, and $d$ is the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49\n"
     ]
    }
   ],
   "source": [
    "print(Gini(extraction_labels(dataset)).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement Gini\n",
    "\n",
    "The Gini Improvement evaluates the discriminative ability of candidate split features $P$ for splitting node $T$. \n",
    "\n",
    "$I(T, P, g) = Gini(y(T)) - \\frac{|T^g_P|}{|T|} Gini(y(T^g_P)) - \\frac{|T^g_{\\neg P}|}{|T|} Gini(y(T^g_{\\neg P}))$\n",
    "\n",
    "where $T^g_P$ contains sequences matching pattern $P$ under the gap constraint, \n",
    "and $T^g_{\\neg P}$ contains the remaining sequences. Higher values indicate \n",
    "better class separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00408\n"
     ]
    }
   ],
   "source": [
    "T_P, T_nP = split_node(dataset, ['c', 'a'], 1)\n",
    "print(improvement_gini(extraction_labels(dataset), T_P, T_nP).round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projected dataset\n",
    "\n",
    "The projected dataset is the subset of the given dataset obtained by taking, for each sequence that contains a given feature, the suffix that starts right after the last element of the feature. Each suffix keeps the original class label, and this reduced dataset is then used to search for possible extensions of the feature more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['b', 'e'], '+1'), (['e'], '-1')]\n"
     ]
    }
   ],
   "source": [
    "print(project_dataset(dataset, ['c','a'], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper Bound\n",
    "Theorem 1: \n",
    "Given a pattern $P$ and a constant $c$, if \n",
    "\n",
    "$UB(P) = max{I(x₁(T_P), 0, ..., 0), I(0, x₂(T_P), 0, ..., 0), ..., I(0, 0, ..., xₐ(T_P))} ≤ c$, \n",
    "\n",
    "then for all super-patterns $J ⊇ P: I(x(T_J)) ≤ c$.\n",
    "\n",
    "For each class $i$, we create an ideal vector where only the count of class $i$ is preserved and all other classes are set to zero, then compute the improvement for this ideal case. The maximum among all these ideal improvements serves as an upper bound for any super-pattern of $P$. This allows the algorithm to safely prune patterns and their extensions when the upper bound is lower than the best improvement found so far, significantly reducing the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.109\n"
     ]
    }
   ],
   "source": [
    "print(upper_bound(dataset, ['c','a']).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Patterns\n",
    "\n",
    "Collects unique symbols from the projected dataset and evaluates each potential pattern extension by computing its Gini improvement on the original dataset. Symbols are sorted by decreasing Gini improvement to enable early pruning of suboptimal patterns through upper bound testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected dataset SubT: [(['b', 'e'], '+1'), (['e'], '-1')]\n",
      "Alphabet: ['b', 'e']\n",
      "T_P: ['+1'], gini = 0.0\n",
      "T_nP: ['+1', '+1', '-1', '-1', '-1', '-1'], gini = 0.4444444444444444\n",
      "Improvement 0.109 of item b , considering pattern ['c', 'a', 'b']\n",
      "\n",
      "\n",
      "T_P: ['+1', '-1'], gini = 0.5\n",
      "T_nP: ['+1', '+1', '-1', '-1', '-1'], gini = 0.48\n",
      "Improvement 0.004 of item e , considering pattern ['c', 'a', 'e']\n",
      "\n",
      "\n",
      "items sorted: [(np.float64(0.10884353741496611), 'b', ['c', 'a', 'b']), (np.float64(0.004081632653061329), 'e', ['c', 'a', 'e'])]\n"
     ]
    }
   ],
   "source": [
    "P = ['c', 'a']\n",
    "g = 0\n",
    "SubT = project_dataset(dataset, P, g)\n",
    "        \n",
    "print(\"Projected dataset SubT:\", SubT)\n",
    "\n",
    "item_improvements = []\n",
    "\n",
    "# Get all unique items in the projected dataset - ALPHABET\n",
    "alphabet = set()\n",
    "for seq, label in SubT:\n",
    "    for item in seq:\n",
    "        alphabet.add(item)\n",
    "        \n",
    "alphabet = sorted(alphabet)\n",
    "        \n",
    "print(\"Alphabet:\", alphabet)\n",
    "\n",
    "for item in alphabet:\n",
    "    P_new = P + [item]\n",
    "    T_P, T_nP = split_node(dataset, P_new, g)\n",
    "    print(f'T_P: {T_P}, gini = {Gini(T_P)}')\n",
    "    print(f'T_nP: {T_nP}, gini = {Gini(T_nP)}')\n",
    "    \n",
    "    if len(T_P) == 0:\n",
    "        continue\n",
    "        \n",
    "    improvement = improvement_gini(extraction_labels(dataset), T_P, T_nP)\n",
    "    item_improvements.append((improvement, item, P_new))\n",
    "    print('Improvement', round(improvement, 3), 'of item', item, ', considering pattern', P_new)\n",
    "    print('\\n')\n",
    "\n",
    "# Sort by improvement \n",
    "item_improvements.sort(key=lambda x: x[0], reverse=True)\n",
    "print('items sorted:', item_improvements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover the Pattern with the Highest Gini Improvement (DPH)\n",
    "\n",
    "The DPH algorithm searches for the most discriminative sequential pattern for splitting a node using a branch-and-bound strategy with a priority queue ordered by Gini improvement.\n",
    "\n",
    "Each candidate pattern is checked against the upper bound; if it fails, the pattern and its extensions are pruned. Otherwise, a projected dataset is built, from which the reduced alphabet is obtained by collecting the unique symbols that appear in the suffix sequences. This reduced alphabet defines the feasible extensions of the current pattern. Each extension is evaluated by splitting the original dataset under the gap constraint and computing its Gini improvement.\n",
    "\n",
    "If the improvement exceeds the current maximum, the best pattern and score are updated, and the extension is added to the queue if it satisfies the length and upper bound conditions.\n",
    "\n",
    "The search terminates when the queue is empty, ensuring discovery of the optimal pattern within the maximum length constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a', 'b'], np.float64(0.261224))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DPH(dataset, g=1, maxL=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SeqDT\n",
    "\n",
    "The SeqDT algorithm performs sequence classification in two stages.\n",
    "\n",
    "First, it uses the Best Tree (BT) procedure to build an initial decision tree by recursively selecting discriminative sequential patterns with DPH and splitting nodes until stopping criteria are reached. This produces a tree that maximizes discrimination on training data.\n",
    "\n",
    "In BT, each node is evaluated for purity using the Gini index.\n",
    "If the node is sufficiently pure, it becomes a leaf with the majority class.\n",
    "Otherwise, DPH finds the best splitting pattern, and the dataset is divided into sequences that contain the pattern and those that do not, under the gap constraint.\n",
    "Recursion continues unless stopping criteria are triggered: too small improvement, too few samples in a child, or exceeding maximum depth.\n",
    "\n",
    "Optionally, SeqDT applies Pessimistic Error Pruning (PEP) to remove subtrees that do not improve generalization, replacing them with leaves when appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'+1': 3, '-1': 4}\n"
     ]
    }
   ],
   "source": [
    "print(sequences_per_class(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"333pt\" height=\"298pt\"\n",
       " viewBox=\"0.00 0.00 333.38 297.94\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 293.94)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-293.94 329.38,-293.94 329.38,4 -4,4\"/>\n",
       "<!-- node_0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>node_0</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"black\" points=\"132.69,-289.94 32.69,-289.94 32.69,-242.94 132.69,-242.94 132.69,-289.94\"/>\n",
       "<text text-anchor=\"middle\" x=\"82.69\" y=\"-276.34\" font-family=\"Arial\" font-size=\"12.00\">Contains [a, b]?</text>\n",
       "<text text-anchor=\"middle\" x=\"82.69\" y=\"-263.34\" font-family=\"Arial\" font-size=\"12.00\">Samples = 7</text>\n",
       "<text text-anchor=\"middle\" x=\"82.69\" y=\"-250.34\" font-family=\"Arial\" font-size=\"12.00\">Gini = 0.490</text>\n",
       "</g>\n",
       "<!-- node_1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>node_1</title>\n",
       "<ellipse fill=\"#1f77b4\" stroke=\"black\" stroke-width=\"2\" cx=\"62.69\" cy=\"-158.7\" rx=\"58.87\" ry=\"33.44\"/>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"62.69\" cy=\"-158.7\" rx=\"62.88\" ry=\"37.47\"/>\n",
       "<text text-anchor=\"middle\" x=\"62.69\" y=\"-168.6\" font-family=\"Arial\" font-size=\"12.00\">Class = +1</text>\n",
       "<text text-anchor=\"middle\" x=\"62.69\" y=\"-155.6\" font-family=\"Arial\" font-size=\"12.00\">Samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"62.69\" y=\"-142.6\" font-family=\"Arial\" font-size=\"12.00\">Gini = 0.000</text>\n",
       "</g>\n",
       "<!-- node_0&#45;&gt;node_1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>node_0&#45;&gt;node_1</title>\n",
       "<path fill=\"none\" stroke=\"green\" d=\"M78.4,-242.75C76.37,-232.03 73.87,-218.81 71.47,-206.1\"/>\n",
       "<polygon fill=\"green\" stroke=\"green\" points=\"74.87,-205.25 69.57,-196.07 67.99,-206.55 74.87,-205.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"83.19\" y=\"-216.94\" font-family=\"Arial\" font-size=\"10.00\">Yes</text>\n",
       "</g>\n",
       "<!-- node_2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>node_2</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"black\" points=\"243.69,-182.2 143.69,-182.2 143.69,-135.2 243.69,-135.2 243.69,-182.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"193.69\" y=\"-168.6\" font-family=\"Arial\" font-size=\"12.00\">Contains [a, d]?</text>\n",
       "<text text-anchor=\"middle\" x=\"193.69\" y=\"-155.6\" font-family=\"Arial\" font-size=\"12.00\">Samples = 5</text>\n",
       "<text text-anchor=\"middle\" x=\"193.69\" y=\"-142.6\" font-family=\"Arial\" font-size=\"12.00\">Gini = 0.320</text>\n",
       "</g>\n",
       "<!-- node_0&#45;&gt;node_2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>node_0&#45;&gt;node_2</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M106.5,-242.75C122.89,-227.14 144.82,-206.25 162.59,-189.33\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"165.09,-191.78 169.92,-182.35 160.26,-186.71 165.09,-191.78\"/>\n",
       "<text text-anchor=\"middle\" x=\"142.19\" y=\"-216.94\" font-family=\"Arial\" font-size=\"10.00\">No</text>\n",
       "</g>\n",
       "<!-- node_3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>node_3</title>\n",
       "<ellipse fill=\"#17becf\" stroke=\"black\" cx=\"123.69\" cy=\"-37.23\" rx=\"58.88\" ry=\"33.47\"/>\n",
       "<text text-anchor=\"middle\" x=\"123.69\" y=\"-47.13\" font-family=\"Arial\" font-size=\"12.00\">Class = &#45;1</text>\n",
       "<text text-anchor=\"middle\" x=\"123.69\" y=\"-34.13\" font-family=\"Arial\" font-size=\"12.00\">Samples = 3</text>\n",
       "<text text-anchor=\"middle\" x=\"123.69\" y=\"-21.13\" font-family=\"Arial\" font-size=\"12.00\">Gini = 0.444</text>\n",
       "</g>\n",
       "<!-- node_2&#45;&gt;node_3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>node_2&#45;&gt;node_3</title>\n",
       "<path fill=\"none\" stroke=\"green\" d=\"M180.36,-134.95C170.91,-118.82 157.98,-96.76 146.86,-77.78\"/>\n",
       "<polygon fill=\"green\" stroke=\"green\" points=\"149.81,-75.89 141.73,-69.03 143.77,-79.42 149.81,-75.89\"/>\n",
       "<text text-anchor=\"middle\" x=\"170.19\" y=\"-95.47\" font-family=\"Arial\" font-size=\"10.00\">Yes</text>\n",
       "</g>\n",
       "<!-- node_4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>node_4</title>\n",
       "<ellipse fill=\"#17becf\" stroke=\"black\" stroke-width=\"2\" cx=\"262.69\" cy=\"-37.23\" rx=\"58.87\" ry=\"33.44\"/>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"262.69\" cy=\"-37.23\" rx=\"62.88\" ry=\"37.47\"/>\n",
       "<text text-anchor=\"middle\" x=\"262.69\" y=\"-47.13\" font-family=\"Arial\" font-size=\"12.00\">Class = &#45;1</text>\n",
       "<text text-anchor=\"middle\" x=\"262.69\" y=\"-34.13\" font-family=\"Arial\" font-size=\"12.00\">Samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"262.69\" y=\"-21.13\" font-family=\"Arial\" font-size=\"12.00\">Gini = 0.000</text>\n",
       "</g>\n",
       "<!-- node_2&#45;&gt;node_4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>node_2&#45;&gt;node_4</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M206.83,-134.95C215.57,-119.82 227.32,-99.48 237.79,-81.35\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"240.83,-83.08 242.8,-72.67 234.77,-79.58 240.83,-83.08\"/>\n",
       "<text text-anchor=\"middle\" x=\"236.19\" y=\"-95.47\" font-family=\"Arial\" font-size=\"10.00\">No</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x11092cc20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = SeqDT(dataset, g = 1, maxL = 3, pru = False, epsilon= 0.15, minS = 0, minN = 2, maxD= 0)\n",
    "\n",
    "dot = plot_tree(tree)\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pessimistic Error Pruning (PEP)\n",
    "\n",
    "Pessimistic Error Pruning (PEP) is a post-pruning method for decision trees that aims to reduce complexity and prevent overfitting by removing branches that don't improve the model's error rate on unseen data.\n",
    "The Pessimistic Error Pruning algorithm is a top-down pruning algorithm.\n",
    "\n",
    "For each node t, we calculate the following:\n",
    "\n",
    "- The error of the node, $e(t)$: is calculated as the sum of misassigned samples (i.e., samples that were not assigned to the majority class) plus a continuity correction of 1/2 in the case of binary decisions. We do this to counteract a bias that can arise when the same set of samples is used for both tree construction and pruning.\n",
    "\n",
    "$e(t)$ = misclassified_samples + 0.5 (continuity correction)\n",
    "\n",
    "- The error of the subtree of the node, $e(T_t)$: is calculated as the sum of the errors of all the leaves following node t (including continuity corrections).\n",
    "\n",
    "$e(T_t)$ = sum of errors of all reachable leaf nodes\n",
    "\n",
    "- The standard error is now calculated from these two results and $n(t)$, which describes the number of samples in node $t$\n",
    "\n",
    "$SE(e(T_t)) = \\sqrt{\\frac{e(T_t)(n(t) - e(T_t))}{n(t)}}$\n",
    "\n",
    "\n",
    "Pruning occurs when the sum of the standard error and the error of the subtree is greater than or equal to the error of the node. This happens because node t, as a leaf node, makes fewer errors than the subtree.\n",
    "\n",
    "$e(t) ≤ e(Tt) + SE$\n",
    "\n",
    "(<https://lamarr-institute.org/blog/pruning-algorithms-mep-pep/>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples = 7, e(t) = 3.5, e(Tt) = 2.5, standard error = 1.27\n",
      "Pruned\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"125pt\" height=\"74pt\"\n",
       " viewBox=\"0.00 0.00 125.38 74.47\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 70.47)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-70.47 121.38,-70.47 121.38,4 -4,4\"/>\n",
       "<!-- node_0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>node_0</title>\n",
       "<ellipse fill=\"#17becf\" stroke=\"black\" cx=\"58.69\" cy=\"-33.23\" rx=\"58.88\" ry=\"33.47\"/>\n",
       "<text text-anchor=\"middle\" x=\"58.69\" y=\"-43.13\" font-family=\"Arial\" font-size=\"12.00\">Class = &#45;1</text>\n",
       "<text text-anchor=\"middle\" x=\"58.69\" y=\"-30.13\" font-family=\"Arial\" font-size=\"12.00\">Samples = 7</text>\n",
       "<text text-anchor=\"middle\" x=\"58.69\" y=\"-17.13\" font-family=\"Arial\" font-size=\"12.00\">Gini = 0.490</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x103fa56d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pru = PEP(tree)\n",
    "dot_pruned = plot_tree(pru)\n",
    "dot_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics\n",
    "\n",
    "Classification performance is evaluated with two complementary metrics. \n",
    "- Accuracy measures the overall proportion of correctly classified sequences, but can be misleading when class distributions are imbalanced. \n",
    "- G-mean is the geometric mean of class-specific recall rates, offering a balanced view of performance across all classes. Unlike accuracy, G-mean is highly sensitive to misclassification of minority classes: if any class has zero recall, the overall score drops to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 0.005s\n",
      "Confusion matrix:\n",
      "[[1 0]\n",
      " [2 0]]\n",
      "Accuracy:  0.3333\n",
      "G-mean:    0.0000\n"
     ]
    }
   ],
   "source": [
    "result = evaluate_dataset(dataset, g=1, maxL=3, pru=False, epsilon=0.15, minS=0, minN=2, maxD=0, visualize= False, show_statistics= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Performance is estimated with stratified 5-fold cross-validation repeated five times using a fixed random seed. Stratification preserves class proportions in each fold, while repetition reduces variance due to specific train–test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4833 ± 0.1167\n",
      "G-mean:   0.0000 ± 0.0000\n"
     ]
    }
   ],
   "source": [
    "result_cv = evaluate_dataset(dataset, method='cv',  g=1, maxL=3, pru=False, epsilon=0.15, minS=0, minN=2, maxD=0,\n",
    "                n_folds=2, n_repeats=5, random_state=42, show_statistics= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
